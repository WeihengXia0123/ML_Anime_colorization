{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset and put into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  45000\n",
      "validation dataset:  5000\n",
      "testing dataset:  10000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import CIFAR-10 dataset into trainset, testset\n",
    "# load in to trainloader, testloader\n",
    "\n",
    "# color dataset\n",
    "transform_color = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "dataset_color = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform_color)\n",
    "\n",
    "batchSize = 64\n",
    "\n",
    "val_size = 5000\n",
    "\n",
    "train_size = len(dataset_color) - val_size\n",
    "\n",
    "trainset_color, validset_color = random_split(dataset_color, [train_size, val_size])\n",
    "\n",
    "trainloader_color = torch.utils.data.DataLoader(trainset_color, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "validloader_color = torch.utils.data.DataLoader(validset_color, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset_color = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform_color)\n",
    "testloader_color = torch.utils.data.DataLoader(testset_color, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "# print the size of train, valid, test datasets\n",
    "print(\"training dataset: \", len(trainset_color))\n",
    "print(\"validation dataset: \", len(validset_color))\n",
    "print(\"testing dataset: \", len(testset_color))\n",
    "\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Convert all rgb images to gray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img, name=\"\"):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.savefig('plot/AE_Conv_500/'+name+\"batch_\"+str(batchSize)+'.png', dpi=1000)\n",
    "    plt.show()\n",
    "\n",
    "if not os.path.exists('./plot/AE_Conv_500'):\n",
    "    os.makedirs('./plot/AE_Conv_500')\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader_color)\n",
    "images_color, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a rgb_img to gray_img\n",
    "def rgb2gray(img):\n",
    "    rgb_img = img.transpose(1,2,0)\n",
    "    gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n",
    "    return gray_img\n",
    "\n",
    "# function to convert a batch of rgb images to gray images\n",
    "def rgb2gray_batch(images):\n",
    "    rgb_images = np.array(images)    \n",
    "    \n",
    "    gray_images = []\n",
    "    for i in range(rgb_images.shape[0]):\n",
    "        img = rgb_images[i]\n",
    "        gray_img = rgb2gray(img)\n",
    "        gray_images.append(gray_img)\n",
    "    \n",
    "    gray_images = np.array(gray_images)\n",
    "    gray_images = torch.from_numpy(gray_images)\n",
    "    return gray_images\n",
    "        \n",
    "images_gray = rgb2gray_batch(images_color)\n",
    "\n",
    "# show images\n",
    "images_gray = images_gray.reshape(batchSize,1,32,32)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images_color), name=\"trainData_color_\")\n",
    "imshow(torchvision.utils.make_grid(images_gray), name=\"trainData_gray_\")\n",
    "print(\"images_gray shape: \", images_gray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Auto-Encoder network\n",
    "The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. Let's define a few variables:\n",
    "\n",
    "K - the number of filters in the convolutional layer\\\n",
    "F - the height and width of the convolutional filters\\\n",
    "S - the stride of the convolution\\\n",
    "P - the padding\\\n",
    "W_in - the width/height (square) of the previous layer\n",
    "\n",
    "Notice that K = out_channels, F = kernel_size, and S = stride. Likewise, W_in is the first and second value of the input_shape tuple.\n",
    "\n",
    "The depth of the convolutional layer will always equal the number of filters K.\n",
    "\n",
    "The spatial dimensions of a convolutional layer can be calculated as: (W_inâˆ’F+2P)/S+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder,self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4,8,kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8,16,kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.ConvTranspose2d(32,16,kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16,8,kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8,3,kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "model = Autoencoder().to(device)\n",
    "print(\"GPU: \", next(model.parameters()).is_cuda)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5)\n",
    "\n",
    "\n",
    "model_path = \"./saved_model/AE_Conv_Train_\"+str(batchSize)+ \".pt\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Continue training from the saved model\")\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard to save all training output logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter(\"./runs/AE_Conv_Train_\"+str(batchSize)+\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "    train_running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for i, data_color in enumerate(trainloader_color, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        images_color, labels_color = data_color\n",
    "        images_gray = rgb2gray_batch(images_color)\n",
    "        \n",
    "        # put data in gpu/cpu\n",
    "        images_color = images_color.to(device)\n",
    "        images_gray = images_gray.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        # Size - images_gray: [batchSize,1,32,32]\n",
    "        images_gray = images_gray.reshape(images_color.shape[0],1,32,32)\n",
    "        outputs = model(images_gray)\n",
    "    \n",
    "        # Backward\n",
    "        # Size - images_color: [batchSize,3,32,32]\n",
    "        # Size - outputs: [batchSize,3,32,32]\n",
    "        loss = criterion(outputs, images_color)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print output statistics\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        j = 400              # print every j mini-batches\n",
    "        if i % j == (j-1):     # print every j mini-batches\n",
    "            print('[%d, %5d] train loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, train_running_loss / j))\n",
    "            \n",
    "            writer.add_scalar('Loss/train', train_running_loss/j, len(trainloader_color)*epoch + i)\n",
    "            train_running_loss = 0.0\n",
    "        \n",
    "    ###################\n",
    "    # validate the model #\n",
    "    ###################\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data_color in enumerate(validloader_color, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images_color, labels_color = data_color\n",
    "            images_gray = rgb2gray_batch(images_color)\n",
    "\n",
    "            # put data in gpu/cpu\n",
    "            images_color = images_color.to(device)\n",
    "            images_gray = images_gray.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            # Size - images_gray: [batchSize,1,32,32]\n",
    "            images_gray = images_gray.reshape(images_color.shape[0],1,32,32)\n",
    "            outputs = model(images_gray)\n",
    "\n",
    "            # Backward\n",
    "            # Size - images_color: [batchSize,3,32,32]\n",
    "            # Size - outputs: [batchSize,3,32,32]\n",
    "            valid_loss = criterion(outputs, images_color)\n",
    "\n",
    "            # print output statistics\n",
    "            valid_running_loss += valid_loss.item()\n",
    "    \n",
    "        print('[%d, %5d] validation loss: %.3f' %\n",
    "              (epoch + 1, i + 1, valid_running_loss/(i+1)))\n",
    "\n",
    "        writer.add_scalar('Loss/valid', valid_running_loss/(i+1), len(trainloader_color)*epoch + i)\n",
    "        valid_running_loss = 0.0\n",
    "    \n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(validloader_color)\n",
    "images_color, labels_color = dataiter.next()\n",
    "print(\"images_color: \", images_color.shape)\n",
    "# show images\n",
    "images_color_show = images_color.reshape(batchSize,3,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_color_show.detach()))\n",
    "\n",
    "images_gray = rgb2gray_batch(images_color)\n",
    "print(\"images_gray: \", images_gray.shape)\n",
    "# show images\n",
    "images_gray = images_gray.reshape(batchSize,1,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_gray.detach()))\n",
    "\n",
    "# run inference on the network\n",
    "# oututs [4,3072]\n",
    "images_gray = images_gray.to(device)\n",
    "outputs = model(images_gray)\n",
    "print(\"outputs: \", outputs.shape)\n",
    "\n",
    "images_color = outputs.reshape(batchSize,3,32,32)\n",
    "images_color = images_color.to(\"cpu\")\n",
    "imshow(torchvision.utils.make_grid(images_color.detach()))\n",
    "\n",
    "images_gray = images_gray.to(\"cpu\")\n",
    "images_gray = torch.cat((images_gray, images_gray, images_gray), 1)\n",
    "final_result_display = torch.cat((images_color_show, images_gray, images_color), 0)\n",
    "imshow(torchvision.utils.make_grid(final_result_display.detach()), name=\"result_onTrainData_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./saved_model'):\n",
    "    os.makedirs('./saved_model')\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(testloader_color)\n",
    "images_color, labels_color = dataiter.next()\n",
    "print(\"images_color: \", images_color.shape)\n",
    "# show images\n",
    "images_color_show = images_color.reshape(batchSize,3,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_color_show.detach()))\n",
    "\n",
    "images_gray = rgb2gray_batch(images_color)\n",
    "print(\"images_gray: \", images_gray.shape)\n",
    "# show images\n",
    "images_gray = images_gray.reshape(batchSize,1,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_gray.detach()))\n",
    "\n",
    "# run inference on the network\n",
    "images_gray = images_gray.to(device)\n",
    "outputs = model(images_gray)\n",
    "outputs_color = outputs.reshape(batchSize,3,32,32)\n",
    "print(outputs_color.shape)\n",
    "\n",
    "outputs_color = outputs.to(\"cpu\")\n",
    "imshow(torchvision.utils.make_grid(outputs_color.detach()))\n",
    "\n",
    "images_gray = images_gray.to(\"cpu\")\n",
    "images_gray = torch.cat((images_gray, images_gray, images_gray), 1)\n",
    "\n",
    "final_result_display = torch.cat((images_color_show, images_gray, outputs_color), 0)\n",
    "imshow(torchvision.utils.make_grid(final_result_display.detach()),name=\"result_onTestData_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# test the model #\n",
    "###################\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_running_loss = 0.0\n",
    "    for i, data_color in enumerate(testloader_color, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        images_color, labels_color = data_color\n",
    "        images_gray = rgb2gray_batch(images_color)\n",
    "\n",
    "        # put data in gpu/cpu\n",
    "        images_color = images_color.to(device)\n",
    "        images_gray = images_gray.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        # Size - images_gray: [batchSize,1,32,32]\n",
    "        images_gray = images_gray.reshape(images_color.shape[0],1,32,32)\n",
    "        outputs = model(images_gray)\n",
    "\n",
    "        # Backward\n",
    "        # Size - images_color: [batchSize,3,32,32]\n",
    "        # Size - outputs: [batchSize,3,32,32]\n",
    "        test_loss = criterion(outputs, images_color)\n",
    "\n",
    "        # print output statistics\n",
    "        test_running_loss += test_loss.item()\n",
    "\n",
    "    print('test loss: %.3f' % (test_running_loss/(i+1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
