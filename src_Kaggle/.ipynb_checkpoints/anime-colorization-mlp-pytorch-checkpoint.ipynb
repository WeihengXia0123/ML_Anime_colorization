{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nImporting the required libraries.''\n\"\"\"\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nimport numpy as np\nimport glob\n\n\"\"\"\nDisplaying the sample sketch and color images.\n\"\"\"\nfor file in glob.glob('../input/anime-sketch-colorization-pair/data/train/*')[:5]:\n    f, a = plt.subplots(1,2, figsize=(10,5))\n    a = a.flatten()\n    \n    img = Image.open(file).convert('RGB')\n    print(img)\n    a[0].imshow(img.crop((0, 0, 512,512))); a[0].axis('off');\n    a[1].imshow(img.crop((512, 0, 1024, 512))); a[1].axis('off');\n    \n    plt.show()\n    print(file)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"'''\n\"\"\"\nCreating a directory for training data. \n\"\"\"\nget_ipython().system('rm -rf trainData_Image/Imagess')\nget_ipython().system('rm -rf trainData_Sketches/Sketches')\nget_ipython().system('rm -rf trainData_Images')\nget_ipython().system('rm -rf trainData_Sketches')\nget_ipython().system('mkdir trainData_Images')\nget_ipython().system('mkdir trainData_Sketches')\nget_ipython().system('mkdir trainData_Images/Images')\nget_ipython().system('mkdir trainData_Sketches/Sketches')\n\"\"\"\nPreprocessing and saving the training data to corresponding directory. \n\"\"\"\n\n\nfor idx, file in tqdm(enumerate(glob.glob('../input/anime-sketch-colorization-pair/data/train/*.png'))):\n    img = Image.open(file).convert('RGB')\n    img.crop((0, 0, 512,512)).save('./trainData_Images/Images/{}.png'.format(idx))\n    img.crop((512, 0, 1024, 512)).save('./trainData_Sketches/Sketches/{}.png'.format(idx))\n\n\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nCreating a directory for validation/test data. \n\"\"\"\nget_ipython().system('rm -rf testData_Images')\nget_ipython().system('rm -rf testData_Sketches')\nget_ipython().system('rm -rf testData_Images/Images')\nget_ipython().system('rm -rf testData_Sketches/Sketches')\nget_ipython().system('mkdir testData_Images')\nget_ipython().system('mkdir testData_Sketches')\nget_ipython().system('mkdir testData_Images/Images')\nget_ipython().system('mkdir testData_Sketches/Sketches')\n\nprint('yo')\n\"\"\"\nPreprocessing and saving the validation/test data to corresponding directory. \n\"\"\"\nfor idx, file in tqdm(enumerate(glob.glob('../input/anime-sketch-colorization-pair/data/val/*.png'))):\n    if (idx ==1):\n        break\n    img = Image.open(file).convert('RGB')\n    img.crop((0, 0, 512,512)).save('./testData_Images/Images/{}.png'.format(idx))\n    img.crop((512, 0, 1024, 512)).save('./testData_Sketches/Sketches/{}.png'.format(idx))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **import datas**"},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls testData_Sketches/Sketches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\ni=0\nfor file in enumerate(glob.glob('./testData_Images/Images/*.png')):\n    i+=1\nfile=glob.glob('./testData_Images/Images/*.png')\nop=[]\nprint(i)  \nfor i in range (0,1):\n    op.append(np.array(Image.open(file[i]).convert('RGB')))\n#print(op)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nfrom torch.utils.data import random_split\nimport glob\n##### import CIFAR-10 dataset into trainset, testset# color dataset\ntransform_color = transforms.Compose([\n        transforms.ToTensor(),\n         transforms.Normalize((0.5), (0.5))\n    ])\n\nbatchSize = 1\n\ntestset_color = torchvision.datasets.ImageFolder(root='./testData_Images/',\n                                        transform=transform_color)\ntestloader_color = torch.utils.data.DataLoader(testset_color, batch_size=batchSize,\n                                          shuffle=False, num_workers=2)\ntestset_sketch = torchvision.datasets.ImageFolder(root='./testData_Sketches/',\n                                        transform=transform_color)\ntestloader_sketch = torch.utils.data.DataLoader(testset_sketch, batch_size=batchSize,\n                                          shuffle=False, num_workers=2)\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# functions to show an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(testloader_color)\nimages_color, labels = dataiter.next()\n\n# show images\nimshow(torchvision.utils.make_grid(images_color))\n\n# show dataiter shape\nprint(images_color.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# functions to show an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(testloader_sketch)\nimages_gray, labels = dataiter.next()\n\n# show images\nimshow(torchvision.utils.make_grid(images_gray))\n\n# show dataiter shape\nprint(images_gray.shape)\nprint(\"images_gray shape: \", images_gray.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the MLP network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nimport glob\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP_net(nn.Module):\n    def __init__(self):\n        # Input layer is 786432, since gray_image is 3x512x512\n        # Output layer is 786432, since color_image is 32x32x3\n        super(MLP_net, self).__init__()\n        \n        self.layer1 = nn.Linear(786432,1024).cuda() #TODO changer Ã§a\n        #self.bn1 = nn.BatchNorm1d(1024).cuda()\n        self.layer2 = nn.Linear(1024,786432).cuda()\n        \n    def forward(self, x):\n        # convert tensor\n        x = x.view(x.size(0), -1)\n        \n        x = self.layer1(x)\n        #x = self.bn1(x)\n        x = self.layer2(x)\n        return x\nprint(\"yo\")\nngpu = 1\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\nprint(device)\nmlp_net = MLP_net().to(device)\n\nprint(\"is cude: \", next(mlp_net.parameters()).is_cuda)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the loss function and optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(mlp_net.parameters(), lr=0.001, momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tensorboard to save all training output logs"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter(\"./runs_mlp/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the network"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_loss=[]\nhistory_val_loss=[]\nfor epoch in range(50):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    num_training = 0\n    for i, data_color in enumerate(testloader_color, 0):\n        for j, data_gray in enumerate(testloader_color, 0):\n            if (i==j):\n                # get the inputs; data is a list of [inputs, labels]\n                images_color, labels_color = data_color\n                images_gray, labels_color = data_gray \n                # put data in gpu/cpu\n                images_color = images_color.to(device)\n                images_gray = images_gray.to(device)\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward + backward + optimize\n                # input: images_gray\n                images_gray = images_gray.view(images_gray.shape[0],-1)\n                print(\"images_gray: \", images_gray.shape)\n                outputs = mlp_net(images_gray)\n                # change the shape of images_color: [batchSize,3,32,32] -> [batchSize,3072]\n                images_color = images_color.reshape(images_color.shape[0], -1) \n                loss = criterion(outputs, images_color)\n                loss.backward()\n                optimizer.step()\n                # print output statistics\n                running_loss += loss.item()\n                j = 200             # print every 200 mini-batches\n                if i % j == (j-1):     # print every 200 mini-batches\n                    print(\"trainingloss:\",'[%d, %5d] loss: %.3f' %\n                          (epoch + 1, i + 1, running_loss / j))\n                    writer.add_scalar('Loss/train', running_loss/j, len(trainset_color)*epoch + i)\n                    history_loss.append(running_loss / j)\n                    running_loss = 0.0\n                break\n    '''\n    running_val_loss = 0.0\n    for i, data_val_color in enumerate(valloader_color, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        images_val_color, labels_val_color = data_val_color\n        images_val_gray = rgb2gray_batch(images_val_color)\n        # put data in gpu/cpu\n        images_val_color = images_val_color.to(device)\n        images_val_gray = images_val_gray.to(device)\n        \n        # forward + backward + optimize\n        # input: images_val_gray\n        images_val_gray = images_val_gray.view(images_val_gray.shape[0],-1)\n        # print(\"images_val_gray: \", images_val_gray.shape)\n        outputs_val = mlp_net(images_val_gray)\n        # change the shape of images_val_color: [batchSize,3,32,32] -> [batchSize,3072]\n        images_val_color = images_val_color.reshape(images_val_color.shape[0], -1) \n        \n        loss_val = criterion(outputs_val, images_val_color)\n        \n        # print output statistics\n        running_val_loss += loss_val.item()\n        \n        \n        j = 10           # print every 200 mini-batches\n        if i % j == (j-1):     # print every 200 mini-batches\n            print(\"validation\",'[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_val_loss / j))\n            writer.add_scalar('Loss/val', running_val_loss/j, len(valset_color)*epoch + i)\n            history_val_loss.append(running_val_loss / j)\n            \n            running_val_loss = 0.0\n    '''\nwriter.flush()\nwriter.close()\nprint('Finished Training')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss graphic evolution displayed (on training set and validation set)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining Figure\nf = plt.figure(figsize=(10,7))\nf.add_subplot()\n\n\n#Adding Subplot\nplt.plot(range(len(history_loss)), history_loss, label = \"loss\") # Loss curve for training set\nplt.plot(range(len(history_val_loss)), history_val_loss, label = \"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\nplt.savefig(\"Loss_curve.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference on train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get some random training images\ndataiter = iter(trainloader_color)\nimages_color, labels_color = dataiter.next()\nprint(\"images_color: \", images_color.shape)\n# show images\nimages_color_show = images_color.reshape(batchSize,3,32,32)\nimshow(torchvision.utils.make_grid(images_color_show.detach()))\n\nimages_gray = rgb2gray_batch(images_color)\nprint(\"images_gray: \", images_gray.shape)\n# show images\nimages_gray = images_gray.reshape(batchSize,1,32,32)\nimshow(torchvision.utils.make_grid(images_gray.detach()))\n\n# run inference on the network\n# oututs [4,3072]\nimages_gray = images_gray.to(device)\noutputs = mlp_net(images_gray)\nprint(outputs.shape)\n\nimages_color = outputs.reshape(batchSize,3,32,32)\nprint(images_color.shape)\n\nimages_color = images_color.to(\"cpu\")\nimshow(torchvision.utils.make_grid(images_color.detach()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference on validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get some random training images\ndataiter = iter(valloader_color)\nimages_color, labels_color = dataiter.next()\nprint(\"images_color: \", images_color.shape)\n# show images\nimages_color_show = images_color.reshape(batchSize,3,32,32)\nimshow(torchvision.utils.make_grid(images_color_show.detach()))\n\nimages_gray = rgb2gray_batch(images_color)\nprint(\"images_gray: \", images_gray.shape)\n# show images\nimages_gray = images_gray.reshape(batchSize,1,32,32)\nimshow(torchvision.utils.make_grid(images_gray.detach()))\n\n# run inference on the network\n# oututs [4,3072]\nimages_gray = images_gray.to(device)\noutputs = mlp_net(images_gray)\nprint(outputs.shape)\n\nimages_color = outputs.reshape(batchSize,3,32,32)\nprint(images_color.shape)\n\nimages_color = images_color.to(\"cpu\")\nimshow(torchvision.utils.make_grid(images_color.detach()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference on test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get some random training images\ndataiter = iter(testloader_color)\nimages_color, labels_color = dataiter.next()\nprint(\"images_color: \", images_color.shape)\n# show images\nimages_color_show = images_color.reshape(batchSize,3,32,32)\nimshow(torchvision.utils.make_grid(images_color_show.detach()))\n\nimages_gray = rgb2gray_batch(images_color)\nprint(\"images_gray: \", images_gray.shape)\n# show images\nimages_gray = images_gray.reshape(batchSize,1,32,32)\nimshow(torchvision.utils.make_grid(images_gray.detach()))\n\n# run inference on the network\n# oututs [batchSize,3072]\nimages_gray = images_gray.to(device)\noutputs = mlp_net(images_gray)\nprint(outputs.shape)\n\nimages_color = outputs.reshape(batchSize,3,32,32)\nprint(images_color.shape)\n\nimages_color = images_color.to(\"cpu\")\nimshow(torchvision.utils.make_grid(images_color.detach()))\n\nimages_gray = images_gray.to(\"cpu\")\nimages_gray = torch.cat((images_gray, images_gray, images_gray), 1)\n\nfinal_result_display = torch.cat((images_color_show, images_gray, images_color), 0)\nimshow(torchvision.utils.make_grid(final_result_display.detach()))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}