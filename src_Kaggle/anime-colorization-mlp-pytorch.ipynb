{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the required libraries.''\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\"\"\"\n",
    "Displaying the sample sketch and color images.\n",
    "\"\"\"\n",
    "for file in glob.glob('../input/anime-sketch-colorization-pair/data/train/*')[:5]:\n",
    "    f, a = plt.subplots(1,2, figsize=(10,5))\n",
    "    a = a.flatten()\n",
    "    \n",
    "    img = Image.open(file).convert('RGB')\n",
    "    print(img)\n",
    "    a[0].imshow(img.crop((0, 0, 512,512))); a[0].axis('off');\n",
    "    a[1].imshow(img.crop((512, 0, 1024, 512))); a[1].axis('off');\n",
    "    \n",
    "    plt.show()\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\"\"\"\n",
    "Creating a directory for training data. \n",
    "\"\"\"\n",
    "get_ipython().system('rm -rf trainData_Image/Imagess')\n",
    "get_ipython().system('rm -rf trainData_Sketches/Sketches')\n",
    "get_ipython().system('rm -rf trainData_Images')\n",
    "get_ipython().system('rm -rf trainData_Sketches')\n",
    "get_ipython().system('mkdir trainData_Images')\n",
    "get_ipython().system('mkdir trainData_Sketches')\n",
    "get_ipython().system('mkdir trainData_Images/Images')\n",
    "get_ipython().system('mkdir trainData_Sketches/Sketches')\n",
    "\"\"\"\n",
    "Preprocessing and saving the training data to corresponding directory. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for idx, file in tqdm(enumerate(glob.glob('../input/anime-sketch-colorization-pair/data/train/*.png'))):\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    img.crop((0, 0, 512,512)).save('./trainData_Images/Images/{}.png'.format(idx))\n",
    "    img.crop((512, 0, 1024, 512)).save('./trainData_Sketches/Sketches/{}.png'.format(idx))\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a directory for validation/test data. \n",
    "\"\"\"\n",
    "get_ipython().system('rm -rf testData_Images')\n",
    "get_ipython().system('rm -rf testData_Sketches')\n",
    "get_ipython().system('rm -rf testData_Images/Images')\n",
    "get_ipython().system('rm -rf testData_Sketches/Sketches')\n",
    "get_ipython().system('mkdir testData_Images')\n",
    "get_ipython().system('mkdir testData_Sketches')\n",
    "get_ipython().system('mkdir testData_Images/Images')\n",
    "get_ipython().system('mkdir testData_Sketches/Sketches')\n",
    "\n",
    "print('yo')\n",
    "\"\"\"\n",
    "Preprocessing and saving the validation/test data to corresponding directory. \n",
    "\"\"\"\n",
    "for idx, file in tqdm(enumerate(glob.glob('../input/anime-sketch-colorization-pair/data/val/*.png'))):\n",
    "    if (idx ==1):\n",
    "        break\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    img.crop((0, 0, 512,512)).save('./testData_Images/Images/{}.png'.format(idx))\n",
    "    img.crop((512, 0, 1024, 512)).save('./testData_Sketches/Sketches/{}.png'.format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **import datas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls testData_Sketches/Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "i=0\n",
    "for file in enumerate(glob.glob('./testData_Images/Images/*.png')):\n",
    "    i+=1\n",
    "file=glob.glob('./testData_Images/Images/*.png')\n",
    "op=[]\n",
    "print(i)  \n",
    "for i in range (0,1):\n",
    "    op.append(np.array(Image.open(file[i]).convert('RGB')))\n",
    "#print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import glob\n",
    "##### import CIFAR-10 dataset into trainset, testset# color dataset\n",
    "transform_color = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "batchSize = 1\n",
    "\n",
    "testset_color = torchvision.datasets.ImageFolder(root='./testData_Images/',\n",
    "                                        transform=transform_color)\n",
    "testloader_color = torch.utils.data.DataLoader(testset_color, batch_size=batchSize,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "testset_sketch = torchvision.datasets.ImageFolder(root='./testData_Sketches/',\n",
    "                                        transform=transform_color)\n",
    "testloader_sketch = torch.utils.data.DataLoader(testset_sketch, batch_size=batchSize,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(testloader_color)\n",
    "images_color, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images_color))\n",
    "\n",
    "# show dataiter shape\n",
    "print(images_color.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(testloader_sketch)\n",
    "images_gray, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images_gray))\n",
    "\n",
    "# show dataiter shape\n",
    "print(images_gray.shape)\n",
    "print(\"images_gray shape: \", images_gray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Input layer is 786432, since gray_image is 3x512x512\n",
    "        # Output layer is 786432, since color_image is 32x32x3\n",
    "        super(MLP_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(786432,1024).cuda() #TODO changer Ã§a\n",
    "        #self.bn1 = nn.BatchNorm1d(1024).cuda()\n",
    "        self.layer2 = nn.Linear(1024,786432).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convert tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "print(\"yo\")\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "mlp_net = MLP_net().to(device)\n",
    "\n",
    "print(\"is cude: \", next(mlp_net.parameters()).is_cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard to save all training output logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter(\"./runs_mlp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loss=[]\n",
    "history_val_loss=[]\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_training = 0\n",
    "    for i, data_color in enumerate(testloader_color, 0):\n",
    "        for j, data_gray in enumerate(testloader_color, 0):\n",
    "            if (i==j):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                images_color, labels_color = data_color\n",
    "                images_gray, labels_color = data_gray \n",
    "                # put data in gpu/cpu\n",
    "                images_color = images_color.to(device)\n",
    "                images_gray = images_gray.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                # input: images_gray\n",
    "                images_gray = images_gray.view(images_gray.shape[0],-1)\n",
    "                print(\"images_gray: \", images_gray.shape)\n",
    "                outputs = mlp_net(images_gray)\n",
    "                # change the shape of images_color: [batchSize,3,32,32] -> [batchSize,3072]\n",
    "                images_color = images_color.reshape(images_color.shape[0], -1) \n",
    "                loss = criterion(outputs, images_color)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print output statistics\n",
    "                running_loss += loss.item()\n",
    "                j = 200             # print every 200 mini-batches\n",
    "                if i % j == (j-1):     # print every 200 mini-batches\n",
    "                    print(\"trainingloss:\",'[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / j))\n",
    "                    writer.add_scalar('Loss/train', running_loss/j, len(trainset_color)*epoch + i)\n",
    "                    history_loss.append(running_loss / j)\n",
    "                    running_loss = 0.0\n",
    "                break\n",
    "    '''\n",
    "    running_val_loss = 0.0\n",
    "    for i, data_val_color in enumerate(valloader_color, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        images_val_color, labels_val_color = data_val_color\n",
    "        images_val_gray = rgb2gray_batch(images_val_color)\n",
    "        # put data in gpu/cpu\n",
    "        images_val_color = images_val_color.to(device)\n",
    "        images_val_gray = images_val_gray.to(device)\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        # input: images_val_gray\n",
    "        images_val_gray = images_val_gray.view(images_val_gray.shape[0],-1)\n",
    "        # print(\"images_val_gray: \", images_val_gray.shape)\n",
    "        outputs_val = mlp_net(images_val_gray)\n",
    "        # change the shape of images_val_color: [batchSize,3,32,32] -> [batchSize,3072]\n",
    "        images_val_color = images_val_color.reshape(images_val_color.shape[0], -1) \n",
    "        \n",
    "        loss_val = criterion(outputs_val, images_val_color)\n",
    "        \n",
    "        # print output statistics\n",
    "        running_val_loss += loss_val.item()\n",
    "        \n",
    "        \n",
    "        j = 10           # print every 200 mini-batches\n",
    "        if i % j == (j-1):     # print every 200 mini-batches\n",
    "            print(\"validation\",'[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_val_loss / j))\n",
    "            writer.add_scalar('Loss/val', running_val_loss/j, len(valset_color)*epoch + i)\n",
    "            history_val_loss.append(running_val_loss / j)\n",
    "            \n",
    "            running_val_loss = 0.0\n",
    "    '''\n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss graphic evolution displayed (on training set and validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Figure\n",
    "f = plt.figure(figsize=(10,7))\n",
    "f.add_subplot()\n",
    "\n",
    "\n",
    "#Adding Subplot\n",
    "plt.plot(range(len(history_loss)), history_loss, label = \"loss\") # Loss curve for training set\n",
    "plt.plot(range(len(history_val_loss)), history_val_loss, label = \"val_loss\") # Loss curve for validation set\n",
    "\n",
    "plt.title(\"Loss Curve\",fontsize=18)\n",
    "plt.xlabel(\"Epochs\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(\"Loss_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader_color)\n",
    "images_color, labels_color = dataiter.next()\n",
    "print(\"images_color: \", images_color.shape)\n",
    "# show images\n",
    "images_color_show = images_color.reshape(batchSize,3,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_color_show.detach()))\n",
    "\n",
    "images_gray = rgb2gray_batch(images_color)\n",
    "print(\"images_gray: \", images_gray.shape)\n",
    "# show images\n",
    "images_gray = images_gray.reshape(batchSize,1,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_gray.detach()))\n",
    "\n",
    "# run inference on the network\n",
    "# oututs [4,3072]\n",
    "images_gray = images_gray.to(device)\n",
    "outputs = mlp_net(images_gray)\n",
    "print(outputs.shape)\n",
    "\n",
    "images_color = outputs.reshape(batchSize,3,32,32)\n",
    "print(images_color.shape)\n",
    "\n",
    "images_color = images_color.to(\"cpu\")\n",
    "imshow(torchvision.utils.make_grid(images_color.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(valloader_color)\n",
    "images_color, labels_color = dataiter.next()\n",
    "print(\"images_color: \", images_color.shape)\n",
    "# show images\n",
    "images_color_show = images_color.reshape(batchSize,3,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_color_show.detach()))\n",
    "\n",
    "images_gray = rgb2gray_batch(images_color)\n",
    "print(\"images_gray: \", images_gray.shape)\n",
    "# show images\n",
    "images_gray = images_gray.reshape(batchSize,1,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_gray.detach()))\n",
    "\n",
    "# run inference on the network\n",
    "# oututs [4,3072]\n",
    "images_gray = images_gray.to(device)\n",
    "outputs = mlp_net(images_gray)\n",
    "print(outputs.shape)\n",
    "\n",
    "images_color = outputs.reshape(batchSize,3,32,32)\n",
    "print(images_color.shape)\n",
    "\n",
    "images_color = images_color.to(\"cpu\")\n",
    "imshow(torchvision.utils.make_grid(images_color.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(testloader_color)\n",
    "images_color, labels_color = dataiter.next()\n",
    "print(\"images_color: \", images_color.shape)\n",
    "# show images\n",
    "images_color_show = images_color.reshape(batchSize,3,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_color_show.detach()))\n",
    "\n",
    "images_gray = rgb2gray_batch(images_color)\n",
    "print(\"images_gray: \", images_gray.shape)\n",
    "# show images\n",
    "images_gray = images_gray.reshape(batchSize,1,32,32)\n",
    "imshow(torchvision.utils.make_grid(images_gray.detach()))\n",
    "\n",
    "# run inference on the network\n",
    "# oututs [batchSize,3072]\n",
    "images_gray = images_gray.to(device)\n",
    "outputs = mlp_net(images_gray)\n",
    "print(outputs.shape)\n",
    "\n",
    "images_color = outputs.reshape(batchSize,3,32,32)\n",
    "print(images_color.shape)\n",
    "\n",
    "images_color = images_color.to(\"cpu\")\n",
    "imshow(torchvision.utils.make_grid(images_color.detach()))\n",
    "\n",
    "images_gray = images_gray.to(\"cpu\")\n",
    "images_gray = torch.cat((images_gray, images_gray, images_gray), 1)\n",
    "\n",
    "final_result_display = torch.cat((images_color_show, images_gray, images_color), 0)\n",
    "imshow(torchvision.utils.make_grid(final_result_display.detach()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
